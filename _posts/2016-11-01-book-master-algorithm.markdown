---
layout: post
title: "마스터 알고리즘: 기계학습의 원리와 미래"
categories: book
---

<img style="float: right; margin: 10px;" src="/assets/book_cover/9791185459547.jpg" width="120px">페드로 도밍고스 교수의 『마스터 알고리즘』(원제: The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World)을 재미있게 읽었다. 저자는 자신이 "마스터 알고리즘"이라고 부르는 궁극의 기계학습 알고리즘을 만들기 위해 현존하는 접근 방식을 통합할 수 있는 방향을 제시하고, 그것이 실제로 구현되었을 때 펼쳐질 미래의 모습을 다소 낙관적인 관점에서 그려놓았다. 이 책을 읽고 얻을 수 있는 세 가지를 정리해 보았다.<!--more-->

### 1. 기계학습의 원리

서로 다른 알고리즘을 통합하려면 먼저 각각에 대해 잘 알아야 한다. 저자는 이 분야의 연구를 접근법에 따라 5개 종족(?)으로 나누는데, 기호주의, 연결주의, 진화주의, 베이지안, 유추주의가 그 주인공이다. 종족마다 한 챕터씩 할애해서 주요한 아이디어와 무기(도구상자), 역사와 발전 과정을 소개한다. 그리고 그 과정에서 기계학습에서 공통적으로 중요한 개념도 자연스럽게 배우도록 구성되었다. 하나씩 살펴보자.

A. 기호주의는 기본적으로 데이터에서 규칙을 찾아내는 학습이다. 지식은 명제로 표현되고, 명제 간의 관계를 파악하고 추론하거나 일반화를 하기에 용이하다.

> 기호주의자의 핵심 믿음은 모든 지성은 기호의 조작으로 귀결될 수 있다는 것이다., 158p

많이 쓰이는 분류기(Classifier) 중에서 규칙 기반 방식에 가까운 것으로는 의사결정나무(Decision Tree)가 있다.

> 퀸란은 기호주의학파에서 가장 유망한 연구자다. 전혀 동요하지 않는 성격이며 뼛속까지 호주 사람인 그는 매년 끈질기게 개선하고 논문을 아름다울 정도로 분명하게 써서 의사결정트리를 분류기 분야의 금본위로 만들었다., 157p

신용카드 발급을 거절할 때 이유를 설명해야 하기 때문에 복잡한 알고리즘 대신 규칙으로 설명할 수 있는 의사결정나무를 쓴다는 건 많이 알려진 이야기다. 아무튼 이런 설명을 따라가다 보면, 기계학습에서 중요한 개념인 과적합(Overfitting)과 [편향-분산 트레이드오프(Bias-Varance Tradeoff)](http://www.4four.us/article/2010/11/bias-variance-tradeoff)가 무엇이며 왜 문제인지도 이해할 수 있다.

B. 연결주의는 최근의 딥러닝 열풍으로 가장 뜨거운 관심을 받고 있는데, 기본 원리인 퍼셉트론(Perceptron)과 역전파(Backpropagation)를 소개하면서 기울기 하강(Gradient Descent)과 국소 최저점(Local Minimum) 문제 등을 설명한다. 아래 인용한 것처럼 발전 역사의 맥락과 해결책의 의미를 쉬운 말로 풀어준다.

> 이것이 민스키와 페퍼트를 포함한 여러 사람들이 다층 퍼셉트론을 학습시키는 방법을 찾지 못한 요인이다. 그들은 계단함수를 S자 곡선으로 교체하고 기울기 하강을 적용하는 방법을 생각했겠지만 곧 오류의 국소 최소점이라는 문제에 부딪혔다. 그 당시 연구자들은 컴퓨터 모의실험을 신뢰하지 않았다. 그들은 알고리즘의 동작에 관한 수학적 증명을 요구했고, 역전파에 관한 수학적 증명은 없었다. 하지만 우리가 깨달은 것은 지역 최소점은 그런대로 쓸 만하다는 점이다., 190p (한 문단 안에서도 "국소 최소점"과 "지역 최소점"을 혼용하고 있다…)

> 자동부호기(autoencoder)는 1980년대에 알려졌지만 은닉 층이 단 하나여도 학습을 수행하기는 매우 어려웠다. 많은 정보를 소량의 정보로 압축하는 방법을 알아내는 일은 지독히 어려운 문제다. (중략) 발견하는 데 10년이 넘게 걸린 계책은 은닉 층을 입력과 출력 층보다 더 크게 만드는 것이었다. 헉? 실제로 이것은 계책의 절반이다. 다른 절반은 몇몇을 제외한 모든 은닉 층이 지정된 시간에 꺼지도록 하는 것이다. 이런 방식은 여전히 은닉 층이 입력을 단순히 복사하는 것을 방지하며 학습을 매우 쉽게 한다., 199p

C. 유전 알고리즘과 유전자 프로그래밍으로 대표되는 진화주의는 학교 수업 때 이후로는 거의 접할 일이 없었는데, 이 책에 나오는 역사 뒷얘기를 들으니까 상황을 좀 알 것도 같다.

> 문제점을 지적하는 비판자가 늘어나자 코자는 1992년에 발간한 "유전자 프로그래밍"(Genetic Programming)에서 유전자 프로그래밍이 부울 방식의 회로 합성 문제에서 임의로 회로를 만들어내는 방식들을 물리치는 실험 결과를 실었다. 하지만 승리의 격차는 작았다. 그 후 1995년 미국 캘리포니아의 레이크 타호에서 열린 국제머신러닝학술대회(ICML)에서 케빈 랑은 코자와 같은 실험 과제에서 언덕 오르기가 유전자 프로그래밍을 때때로 큰 격차로 물리친 결과를 담은 논문을 발표했다. 코자오 다른 진화주의자들은 머신러닝 분야의 주무대인 ICML에 논문을 발표하려고 여러 차례 시도했지만 실험 검증 부분이 충분하지 않다는 이유로 계속 거절당하자 절망이 커져 갔다. (중략) 코자는 랑의 결론을 반박하고 ICML 논문 심사관들의 과학적 만행을 비난하는 23쪽 분량의 논문을 두 단의 ICML 논문 형식으로 작성했다. 그리고 논문의 복사본을 학술 회의가 열리는 강당의 모든 의자에 올려 놓았다., 226 - 227p

D. 다음은 유명한 베이지안이다. 사전확률과 사후확률의 개념은 기본이고, 드디어 단순하면서도 강력한 분류기인 나이브 베이즈(Naive Bayes) 분류기가 나온다. 베이즈 네트워크와 이를 학습하는 방식인 [MCMC](http://www.4four.us/article/2014/11/markov-chain-monte-carlo)와 신뢰 전파(Belief Propagation)의 개념이 어떤 맛인지 보여준다.

> 나이브 베이즈 분류기와 마르코프 연쇄, HMM(은닉 마르코프 모형)은 모두 베이즈 네트워크의 특별한 경우다. (중략) 베이즈주의자에게 베이지안 네트워크는 기호주의자에게 논리가 차지하는 위치와 같다. 즉 어지러울 정도로 다양한 상황을 공통어처럼 멋지게 표현하고 그 모든 경우에서 한결같이 작동하는 알고리즘을 만들어 낸다., 262 - 263p

E. 마지막은 이름만 봐서는 좀 어려운 유추주의인데, 아이템 간의 유사도에 기반한 방식인 최근접 이웃(Nearest Neighbor) 알고리즘과 서포트 벡터 머신(Support Vector Machine)이 등장한다. 유사도를 계산할 때 문제를 일으키는 원인의 하나로서 차원의 저주(Curse of Dimensionality)도 느껴볼 수 있다.

> 1967년 톰 커버와 피터 하트는 데이터가 충분하면 최근접 이웃 알고리즘은 최악의 경우라도 오류를 일으키는 정도가 가상의 최고 분류기의 단지 두 배에 불과하다는 것을 증명했다., 303p

내가 분석과 모델링을 하면서 중요하게 생각하고 또 재미를 느끼는 부분을 아래와 같이 설명하고 있었다.

> 유사성 함수는 머신러닝 알고리즘이 이미 아는 사례를 어떻게 일반화하여 새로운 예들을 판단할 것인가를 제어한다. 이 부분이 바로 문제 영역에 대한 우리의 지식을 머신러닝에 넣는 위치이며 흄의 질문에 대한 유추 알고리즘의 대답이 된다., 320p

위에서 인용한 문장들을 보면 대략 감이 잡힐 텐데, 저자는 구체적인 문제를 제시하고 기계학습이 어떤 원리로 그 문제를 해결하는지를 최대한 일상적인 말로 풀어서 설명한다. 관련 지식이 있다면 '아, 이 개념을 저렇게 설명하는구나' 하면서 이해도를 높이고 큰 그림을 다시 생각해보는 계기가 될 것 같은데, 아예 배경 지식 없다면 추상적으로 보일 수는 있을 것 같다.

### 2. 그래서 통합 알고리즘은?

다섯 종족을 살펴봤으니 이들을 어떻게 통합할 수 있을지가 궁금해진다. 그 전에 한 가지 더 알아야 할 게 있다. 비지도학습, 즉 선생님없이 배우기다. 군집화(클러스터링) 알고리즘(k-means같은)부터 차원 축소(PCA같은), 강화학습(Reinforcement Learning), 관계형 학습(Relational Learning)까지 쭉 훑은 후에 드디어 저자가 생각하는 마스터 알고리즘으로 들어간다. 결론부터 말하면, 마코프 로직 네트워크(MLN)를 이용해서 모형을 표현하고 진화 알고리즘과 기울기 하강을 잘 조합해서 구조와 가중치를 학습하는 것 같다. 그 비전을 담아서 실제로 [알케미](https://alchemy.cs.washington.edu/)라는 프로젝트를 열심히 진행 중이라고 한다.

이런 비전에 대한 다른 전문가들의 의견이 궁금했다. 책 앞뒤에 있는 유명인들의 추천사를 자세히 살펴봤는데, 저자의 마스터 알고리즘 방향성에 대해서는 (에릭 시겔의 "이 분야의 가장 유망한 연구 방향을 제시하여 만족감을 선사한다"를 제외하면) 딱히 언급이나 평가를 하지 않는다. 모두 기계학습에 대해 잘 안내한다거나 기술의 미래를 보여준다는 데에 촛점을 맞추고 있었다.

개인적으로 마스터 알고리즘까지는 잘 모르겠지만, 규칙과 확률의 통합 모델은 매력적이고 잘만 되면 매우 실용적이겠다는 생각이 들었다. 예측 모델링을 하다 보면 단순함과 설명가능성을 유지하기 위해 도메인 지식을 하나의 피처값으로 만들어 넣고 적당한 가중치를 찾을 때가 많다. 단순함을 얻은 대가로 오류율이 높아진다. 신경망같이 복잡한 모델을 쓰게 되면 평균 정확도는 올라갈지 몰라도 오류가 나왔을 때 그 이유가 무엇이고 어떻게 교정할지 찾기는 어렵다. 알파고의 이상한 한 수가 나왔을 때 반응을 생각해보면 된다. 만약 여기에 로직이 들어가게 되면, 우리가 도메인 지식을 알려주기도 쉽고 알고리즘의 결정에 대해 이해하기도 (적어도 지금보다는) 쉽고, 더 나아가서 대화까지도 가능해지지 않을까?

> 인간: 너 왜 이렇게 했어?

> 기계: 이건 이렇고, 저건 저래서요

> 인간: 이게 이런 건 맞는데, 저건 저렇지 않고, 그거가 그래.

> 기계: 아!

대충 이런 느낌?

### 3. 기계학습이 펼쳐질 미래

여기까지 열심히 공부했으니 이제 스스로 배울 수 있는 똑똑한 알고리즘, 인공 지능과 함께 할 미래를 살펴보자. 가장 먼저 나에 대해서 잘 알고 나를 대신해 세상과 협상해줄 대리자(에이전트)가 나올 것이다. 그런 대리자를 가진 것은 나만이 아닐 테니 대리자 프로그램들끼리 알아서 협의한 뒤 내가 누구를 만나 어디를 가서 무엇을 할지를 결정, 아니 추천해줄 것이다. 대리자가 나와 소통하면서 끊임없이 업데이트되는 것은 기본이다. 그러자면 믿고 나의 데이터를 맡길 수 있는 곳이 필요하다. 나와 이해관계가 상충될 위험이 있는 곳은 안 된다. 개인의 취향과 습관이 고스란히 담긴 원천 데이터를 구글이나 아마존같은 기업에 맡기고 싶지는 않을 것이다. 그럼 대안은?

> 당신의 데이터를 맡는 새로운 종류의 회사다. 당신의 돈을 맡는 은행과 같다. 은행은 당신의 돈을 훔치지 않는다., 436p 

> 혹은 당신의 데이터를 집에 있는 하드디스크에 모두 보관하고 회사는 단지 소프트웨어만 빌려줄 수 있다., 439p

나는 여태껏 그리고 지금도 후자를 선호하지만, 전자의 방식도 가능은 하겠구나 싶다. 기계가 일자리를 대체하는 문제에 대한 생각은 어떨까?

> 전환기에는 떠들썩하겠지만 다행히 민주주의 덕분에 행복한 결말이 될 것이다(투표를 소중히 여겨라. 당신의 가장 귀중한 보물일 것이다). 실업률이 50퍼센트를 넘어서거나 그 전이라도 재분배에 관한 태도는 급진적으로 바뀔 것이다. (중략) 결국 우리는 실업률 대신 고용률을 이야기하기 시작하고 고용률의 감소를 발전의 지표로 여길 것이다. 실업급여는 모든 사람에게 주는 기초 수입으로 대체될 것이다., 445p

여기까지는 고개를 끄덕이며 읽었는데, 로봇 전쟁이나 스카이넷(...) 이슈에 대해서는 지나치게 낙관적이지 않은가 하는 생각이 들었다. 로봇 군대를 금지하는 게 실현가능하지 않다는 데는 동의하지만, 로봇끼리 전쟁을 하면 사람이 직접적으로 다치거나 죽지 않으니까 더 낫다고 말할 수 있을까? 지금 영토 분쟁 중이거나 위기인 국가에 로봇 군대가 있다면 어떨지 상상해봤다. "앗.. 우리 로봇 군대가 졌구나. 그럼 우리는 이 땅을 양보하고 떠날게" 적어도 이렇게 되지는 않을 것 같다.

> 마스터 알고리즘을 갖춘 인공 지능이 세상을 지배할 가능성은 0이다. 그 이유는 간단하다. 인간과 달리 컴퓨터에는 그만의 고유한 의지가 없다., 451p

이 부분도 마찬가지. 사람이 명시적으로 가르쳐주지 않은 것까지 스스로 찾아서 학습하는 알고리즘을 앞에서 그토록 강조했는데, 이렇게 단언할 수 있는가 의문이 들었다. 영화에서 그려지는 스카이넷은 물론 극적으로 과장되었다고 보지만 -상상력을 발휘해서- 지구 환경을 깨끗하게 보존하는 것을 타겟으로 학습된 알고리즘이 보기에 인간의 존재가 도움이 안 된다고 추론할 가능성이 전혀 없다고는...

말할 수 있을지 없을지 알기 위해 열심히 공부해보자.


### 노란 형광펜

- 학습은 중요한 부분을 기억하는 만큼 세부 항목은 잊는 것이다., 133p
- 이전에 접하지 않은 데이터에 대한 정확도는 매우 엄중한 평가 기준이다. 사실 많은 과학적 가설이 이 시험을 통과하지 못하는 것처럼 머신러닝의 경우도 그러하다. 과학은 예측만을 위한 것이 아니기 때문에 시험에 통과하지 못했다고 쓸모없는 것은 아니다. 과학은 설명하고 이해하기 위한 것이기도 하다. 하지만 궁극적으로 당신의 모형이 새로운 데이터에 대하여 정확한 예측을 하지 못하면 당신이 근본 현상에 관하여 진실로 이해했거나 납득할 만한 설명을 얻었다고 확신할 수 없다, 139p
- 퀸란은 기호주의학파에서 가장 유망한 연구자다. 전혀 동요하지 않는 성격이며 뼛속까지 호주 사람인 그는 매년 끈질기게 개선하고 논문을 아름다울 정도로 분명하게 써서 의사결정트리를 분류기 분야의 금본위로 만들었다., 157p
- 데이비드 애클리와 제프 힌튼, 테리 세이노브스키는 홉필드가 밝혀낸 신경망의 결정론적 신경세포를 확률론적 신경세포로 교체했다. 신경망의 여러 상태는 일정한 확률로 분포하고 더 높은 에너지 상태는 더 낮은 에너지 상태보다 기하급수로 확률이 작다. 사실 신경망이 특정한 상태에 있을 확률은 열역학에서 잘 알려진 볼츠만 분포를 따르기 때문에 자신들의 신경망을 볼츠만 기계라고 불렀다., 179p
- 마스터 알고리즘은 유전자 프로그래밍도 역전파도 아니지만 구조 학습과 가중치 학습이라는 양쪽의 핵심 요소를 포함해야만 한다., 229p
- 베이즈주의자에게 진실 같은 것은 없다. 그저 가설들의 사전 확률 분포가 있고 데이터를 본 이후에는 베이즈 정리에 따라 사후 확률 분포를 얻는 것이 전부다., 274p
- 바프닉과 동료들은 SVM의 응용 분야를 찾아보다가 곧 손으로 쓴 숫자의 인식 문제에 착수했다. 이 분야에서는 벨연구소의 연결주의자 동료들이 줄곧 세계적인 전문가였다. 모든 사람이 놀랍게도 SVM은 이 과제를 시작하자마자 여러 해를 거치며 숫자 인식을 위해 조심스럽게 설계된 다층 퍼셉트론만큼 좋은 성능을 발휘했다. 이 일을 시작으로 둘 사이의 경쟁 무대가 마련되어 오랫동안 넓은 영역에 걸쳐 경쟁이 벌어졌다. (중략) 하지만 연결주의자들은 싸워보지도 않고 항복하지는 않았다. 벨연구소에서 바프닉이 속한 부서의 관리자였던 래리 재컬은 1995년 저녁 식사를 걸고 신경망이 2000년까지는 SVM만큼 잘 파악될 수 있을 것이라고 바프닉과 내기했다. 그런데 래리가 졌다. 이번에는 바프닉이 2005년까지 더 이상 아무도 신경망을 쓰지 않을 거라는 데 내기를 걸었고 그도 지고 말았다(공짜 저녁을 먹은 사람은 그들의 증인인 얀 르쿤이다). 더욱이 딥 러닝이 출현하면서 연결주의가 다시 우위에 올라섰다., 317-318p (대가들의 내기 에피스드. 논문이나 교과서만 봐서는 근엄하기 그지없을 것 같은데, 이러고 노는(?) 걸 보면 괜히 친근하고 좋더라.)
- 반면 과학자의 장기 전망은 그리 밝지 않다. 미래에는 유일한 과학자가 과학을 연구하는 컴퓨터를 뜻하는 컴퓨터과학자일 것이다. 이전에 나처럼 과학자라고 공식적으로 알려진 사람들은 컴퓨터가 이룬 과학의 진보를 이해하며 인생을 바칠 것이다., 444p
- 머신러닝을 통해 우리가 얻는 가장 큰 혜택은 머신러닝이 배운 지식이 아니라 머신러닝을 가르치며 우리가 배운 것이 된다., 448p
